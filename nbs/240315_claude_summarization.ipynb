{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "backend_path = '../backend'\n",
    "if backend_path not in sys.path:\n",
    "        sys.path.append(backend_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, select, values, update, and_, exists, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from dotenv import load_dotenv\n",
    "from app.models.models import Notice, ResourceLink\n",
    "from app.models.schema import NoticeBase, ResourceLinkBase\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "POSTGRES_PASSWORD = os.environ.get(\"POSTGRES_PASSWORD\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "DATABASE_URL = \"postgresql+psycopg2://airflow:airflow@localhost:5432/airflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "selected_date = pendulum.now(\"utc\").subtract(days=1).strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_date = pendulum.now().subtract(days=1).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_in_corpus(input:str, encoding_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(input))\n",
    "    return num_tokens\n",
    "\n",
    "def est_costs(price_input_mil: float = 10.0, price_output_mil: float = 30.0, len_input: int = 0) -> float:\n",
    "    price_per_token_input = price_input_mil / 1000000\n",
    "    price_per_token_output = price_output_mil / 1000000\n",
    "    print(f\"Cost of input: {len_input * price_per_token_input}; Cost of output: {len_input * price_per_token_output}\")\n",
    "\n",
    "def haiku_cost(num_tokens: int) -> str:\n",
    "    return est_costs(.25, 1.25, num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SessionLocal() as db:\n",
    "    stmt = text(\"\"\"select id, text from resource_links \n",
    "                    where notice_id in \n",
    "                        (select id from notices\n",
    "                            where\n",
    "                            naics_code_id = \n",
    "                                (select id from naics_codes where \\\"naicsCode\\\" = 236220)\n",
    "                                and\n",
    "                                \\\"postedDate\\\" = :prior_date)\n",
    "                    and \n",
    "                    text != 'unparsable' \n",
    "                    and\n",
    "                    text != 'adobe-error'\n",
    "                    and\n",
    "                    text != 'encoding-error'\n",
    "                    and\n",
    "                    text is not null\n",
    "                \"\"\") \n",
    "    results = db.execute(stmt, params={\"prior_date\": prior_date}).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_group = results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Spanish.\",  # <-- system prompt\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, Claude!\"}],  # <-- user prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01VkS1q7s2DQWFowWK9MtgSQ', content=[ContentBlock(text='¡Hola! Es un placer saludarte. Responderé en español como me lo has pedido.', type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=17, output_tokens=31))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claude_text_summarization(text: str, max_tokens: int = 1000, temperature: float = 0.0, model: str = 'claude-3-sonnet-20240229'):\n",
    "    client = anthropic.Anthropic()\n",
    "    model = model\n",
    "    current_time = pendulum.now().strftime(\"%Y%m%d:%H%M%S\")\n",
    "    max_tokens = max_tokens\n",
    "    temperature = temperature\n",
    "    system = \"You are a highly skilled AI trained to analyze text and summarize it very succinctly.\"\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Analyze the provided government contracting document to extract key information that will help contractors assess whether the project aligns with their capabilities and is worth pursuing. Focus on the following aspects:\n",
    "\n",
    "            1. Scope of Work: What specific services or products does the project require?\n",
    "            2. Special Equipment Needed: Are there unique tools or machinery necessary for project completion?\n",
    "            3. Domain of Expertise Required: What specialized knowledge or skills are needed?\n",
    "            4. Contractor Workforce Size: Estimate the workforce size needed to meet project demands.\n",
    "\n",
    "            Additionally, consider these factors to further refine suitability assessment:\n",
    "            - Project Duration: How long is the project expected to last?\n",
    "            - Location and Logistics: Where is the project located, and are there significant logistical considerations?\n",
    "            - Budget and Payment Terms: What is the budget range, and how are payments structured?\n",
    "            - Compliance and Regulations: Are there specific industry regulations or standards to comply with?\n",
    "            - Past Performance Requirements: Is prior experience in similar projects a prerequisite?\n",
    "\n",
    "            Summarize these elements in no more than 25 sentences to provide a comprehensive overview, enabling contractors to quickly determine project compatibility and feasibility. Highlight any potential challenges or requirements that may necessitate additional considerations.\n",
    "            Text is below:\n",
    "            {text}\n",
    "            \"\"\" \n",
    "    },\n",
    "    ]\n",
    "    res = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        system=system,\n",
    "        messages=messages)\n",
    "    completion_tokens = res.usage.output_tokens\n",
    "    prompt_tokens = res.usage.input_tokens\n",
    "    total_tokens = completion_tokens + prompt_tokens\n",
    "    data = {\n",
    "        \"Model\": model,\n",
    "        \"Completion Tokens\": completion_tokens,\n",
    "        \"Prompt Tokens\": prompt_tokens,\n",
    "        \"Total Tokens\": total_tokens,\n",
    "        \"Prompt\": messages,\n",
    "        \"Temperature\": temperature,\n",
    "        \"Max_Tokens\": max_tokens,\n",
    "        \"Response\": res.content[0].text,\n",
    "    }\n",
    "    with open(f\"./completions/{current_time}-{model}.json\", \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    return res.content[0].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
